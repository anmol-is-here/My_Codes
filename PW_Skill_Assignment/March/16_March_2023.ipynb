{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANSWER-1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OVER-FITTING\n",
    "This leads to a model that performs very well on the training data but poorly on new, unseen data. The consequences of over-fitting are reduced accuracy and poor generalization.\n",
    "\n",
    "\n",
    "#### UNDER-FITTING\n",
    "This leads to poor performance on both the training data and new, unseen data. The consequences of under-fitting are reduced accuracy and poor model performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANSWER-2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over-fitting can be reduced by the following techniques:-\n",
    "\n",
    "1.Dropout\n",
    "\n",
    "Dropout is a regularization technique that randomly drops out some neurons during training, which helps to prevent over-fitting. This technique encourages the model to learn multiple independent features and reduces the dependency between neurons.\n",
    "\n",
    "2.Data augmentation\n",
    "\n",
    "Data augmentation is a technique that artificially increases the size of the training data by applying various transformations to the data, such as rotation, scaling, and flipping. This helps the model to learn a wider range of patterns and reduces over-fitting.\n",
    "\n",
    "3.Cross-validation\n",
    "\n",
    "Cross-validation is a technique that splits the data into training and validation sets, trains the model on the training set and evaluates its performance on the validation set. This helps to identify over-fitting and select the best model that generalizes well on new, unseen data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANSWER-3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underfitting leads to poor performance on both the training data and new, unseen data. The consequences of under-fitting are reduced accuracy and poor model performance.\n",
    "\n",
    "Scenarios in which wnderfitting can occur are:-\n",
    "\n",
    "1.High bias.\n",
    "\n",
    "2.Inappropriate feature selection.\n",
    "\n",
    "3.Insufficient training data.\n",
    "\n",
    "4.Model complexity.\n",
    "\n",
    "5.Incorrect model architecture."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANSWER-4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bias-variance tradeoff is a fundamental concept in machine learning that refers to the tradeoff between a model's ability to fit the training data well and its ability to generalize well to new, unseen data.\n",
    "\n",
    "Bias refers to the difference between the true value and the predicted value of the model. A model with high bias tends to be too simple and unable to capture the underlying pattern in the data.\n",
    "\n",
    "Variance refers to the sensitivity of the model to the noise in the training data. A model with high variance tends to over-fit the training data and capture the noise in the data, leading to poor performance on new and unseen data.\n",
    "\n",
    "#### Relationship between bias and variance:-\n",
    "As the complexity of the model increases, the bias decreases, and the variance increases. On the other hand, as the complexity of the model decreases, the bias increases, and the variance decreases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANSWER-5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Using learning curves-->\n",
    "\n",
    "Learning curves show how the model's performance improves as the size of the training data increases. If the learning curve plateaus, it indicates that the model is unable to learn from additional training data, and the model may be under-fitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Using training and validation curves\n",
    "\n",
    "Plotting the training and validation curves of a model can help detect over-fitting and under-fitting. If the training error is much lower than the validation error, it indicates that the model is over-fitting. If both the training and validation errors are high, it indicates that the model is under-fitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Using cross-validation\n",
    "\n",
    "Cross-validation is a technique for evaluating the performance of a model on multiple subsets of the training data. If the model performs well on all the subsets, it indicates that the model is not over-fitting. If the performance is poor on all subsets, it indicates that the model is under-fitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine whether a model is over-fitting or under-fitting, we can use the above methods to analyze the model's performance. If the training error is low, but the validation error is high, it indicates that the model is over-fitting. If both the training and validation errors are high, it indicates that the model is under-fitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANSWER-6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias and variance are two sources of error in machine learning models. Bias refers to the difference between the expected output and the true output of the model, while variance refers to the variability of the model's output for different inputs.\n",
    "\n",
    "==> **High bias models** are typically too simple and unable to capture the underlying patterns in the data. They tend to under-fit the data, leading to high training and test errors. High bias models have low complexity and often have fewer parameters than the data requires.\n",
    "\n",
    "**Examples** of high bias models include linear regression models, which assume that the relationship between the inputs and the output is linear, even when it is not.\n",
    "\n",
    "==> **High variance models** are typically too complex and able to fit the training data too closely, including noise in the data. They tend to over-fit the data, leading to low training error but high test error. High variance models have high complexity and often have more parameters than necessary.\n",
    "\n",
    "**Examples** of high variance models include decision trees with deep and complex branches, which can fit the training data too closely.\n",
    "\n",
    "The main difference between high bias and high variance models is their performance. High bias models perform poorly on both training and test data, while high variance models perform well on training data but poorly on test data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANSWER-7\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is a technique used in machine learning to prevent over-fitting, which occurs when a model is too complex and performs well on the training data but poorly on new, unseen data. Regularization works by adding a penalty term to the loss function that encourages the model to have smaller weights, making it less complex and more likely to generalize well to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
